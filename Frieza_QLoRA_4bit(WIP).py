# -*- coding: utf-8 -*-
"""Frieza-QLoRA-4Bit

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bkpzq-9-jQSItZDRydAGV0ZkPMAVq7wG
"""

# ==============================================================================
# CELL 1: Installation **After Installation Restart Is Required**
# ==============================================================================
"""
Custom GRPO QLoRA Trainer for Qwen2.5-14B-Instruct for PnL Maximization (Colab Ready)
*** USING UNSLOTH & USER'S CUSTOM TRAINER LOGIC ***

This script uses a custom GRPOTrainer implementation provided by the user,
integrated with Unsloth for optimization and QLoRA for memory efficiency.

IMPORTANT:
- Ensure the custom reward logic in `calculate_trade_reward` matches your goals.
- Verify Google Drive paths.
- Ensure sufficient VRAM (>18GB recommended).

--- Colab Setup Instructions ---
1. Runtime Selection: Go to Runtime > Change runtime type. Select GPU (A100 recommended).
2. Run Installation Cell (This Cell): Execute first.
   IMPORTANT: You MUST restart the runtime after this cell finishes.
3. Run Setup Cell (Next Cell): Execute to mount Drive and check versions.
4. Verify Paths & Configs in Cell 3.
5. Run Training: Execute Cell 3.
"""

print("CELL 1: Installing required libraries with Unsloth...")
# Use -q for quieter output

# --- Installation Strategy ---
# 1. Install Unsloth first using their recommended method for Colab.
#    Unsloth often bundles compatible core dependencies (torch, transformers, peft, bitsandbytes).
# 2. Install any remaining libraries needed by the custom trainer logic.

# Install Unsloth from PyPI (latest stable version)
!pip install -q unsloth==2025.3.19

# Install other potentially needed libraries (Unsloth might already include some)
# Pin protobuf for known compatibility issues. Add pandas for custom code.
!pip install -q --force-reinstall \
   trl \
   datasets \
   accelerate \
   tensorboard \
   protobuf==3.20.* \
   pandas

# Note: torch, transformers, peft, bitsandbytes, triton should be handled by unsloth install.
# We check versions in Cell 2.

# ==============================================================================
# CELL 2: Setup and Version Check
# ==============================================================================
print("\nCELL 2: Running Setup and Version Check...")

import traceback
import os
import sys # Import sys for handler check

# --- Mount Google Drive ---
try:
    # Check if running in Colab
    if 'google.colab' in sys.modules:
        from google.colab import drive
        print("Mounting Google Drive...")
        drive.mount('/content/drive', force_remount=True)
        print("Google Drive mounted successfully.")
    else:
        print("Not running in Colab, skipping Google Drive mount.")
except ImportError:
    print("Google Colab `drive` import failed. Assuming not in Colab.")
except Exception as e:
    print(f"Error mounting Google Drive: {e}")
    print("Ensure you authorize access when prompted.")

# --- Import Core Libraries FOR VERSION CHECK ONLY ---
# Import Unsloth FIRST to check installation
try:
    import unsloth
    print(f"Successfully imported Unsloth version: {unsloth.__version__}")
except ImportError:
    print("!!! ERROR: Unsloth not installed correctly or import failed. Please check Cell 1 and ensure runtime was restarted. !!!")
    exit()
except Exception as e:
    print(f"Error importing Unsloth: {e}")
    traceback.print_exc() # Print detailed traceback for import error
    exit()

# Import other libraries (some might fail if not installed yet)
try:
    import torch
    import transformers
    import datasets
    import trl
    import peft
    import bitsandbytes # Import to trigger potential setup errors here
    import accelerate
    import google.protobuf
    # Triton might be optional depending on Unsloth version and hardware
    try:
        import triton
        triton_available = True
    except ImportError:
        print("Triton not found (might be okay).")
        triton_available = False
    import pandas as pd # Check pandas

    print("\n--- Library Version Check ---")
    print(f"PyTorch Version: {torch.__version__}")
    print(f"PyTorch CUDA Version: {torch.version.cuda}")
    print(f"Transformers Version: {transformers.__version__}")
    print(f"Datasets Version: {datasets.__version__}")
    print(f"TRL Version: {trl.__version__}")
    print(f"PEFT Version: {peft.__version__}")
    print(f"Bitsandbytes Version: {bitsandbytes.__version__}")
    print(f"Accelerate Version: {accelerate.__version__}")
    print(f"Protobuf Version: {google.protobuf.__version__}")
    if triton_available:
        try:
            # Triton version attribute might not exist in older versions
            if hasattr(triton, '__version__'):
                 print(f"Triton Version: {triton.__version__}")
            else:
                 print(f"Triton imported, but version attribute not found.")
        except Exception as e_triton_ver:
            print(f"Could not get Triton version: {e_triton_ver}")
    print(f"Unsloth Version: {unsloth.__version__}") # Already checked import above
    print(f"Pandas Version: {pd.__version__}")
    print("-----------------------------")

except ImportError as e:
    print(f"!!! WARNING: Failed to import some libraries for version check: {e} !!!")
    print("!!! This might be okay if you haven't run the second install step yet. !!!")
except AttributeError as ae:
     print(f"!!! ERROR: AttributeError during import/check: {ae} !!!")
     traceback.print_exc()
     exit()
except Exception as e:
    print(f"An unexpected error occurred during import/version check: {e}")
    traceback.print_exc()
    exit()

# --- Check GPU Availability and Setup Compute Type ---
try:
    import torch
    if not torch.cuda.is_available():
        print("!!! ERROR: No CUDA GPU detected! Check Colab Runtime Type. !!!")
        exit()
    else:
        print(f"CUDA Detected: {torch.cuda.get_device_name(0)}")
        if torch.cuda.is_bf16_supported():
            print("bfloat16 is supported. Using bfloat16.")
            compute_dtype = torch.bfloat16
            use_bf16 = True
            use_fp16 = False
        else:
            print("bfloat16 not supported. Using fp16.")
            compute_dtype = torch.float16
            use_bf16 = False
            use_fp16 = True
except Exception as e:
     print(f"Error during GPU check / compute type setup: {e}")
     exit()

print("\nSetup and Version Check Complete.")
print("If Unsloth imported successfully, run the second pip install command in a new cell, then proceed to Cell 3.")

# ==============================================================================
# CELL 3: Custom Trainer Definitions (Run AFTER Cell 2)
# ==============================================================================
print("\nCELL 3: Defining Custom Classes and Functions...")

# --- Imports needed for these definitions ---
import os
import sys
import json
import torch
import random
import argparse
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
from tqdm import tqdm
import logging
import re
from copy import deepcopy
from datasets import load_dataset, Dataset
import torch.nn.functional as F
from typing import Dict, List, Tuple, Optional, Any
import pandas as pd
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader

# Import Unsloth FIRST - before any transformers imports
from unsloth import FastLanguageModel

from transformers import (
    AutoModelForCausalLM, # Needed for type hint? Or within GRPOTrainer? Keep for now.
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments, # Used by custom trainer
    DataCollatorForLanguageModeling, # Imported but not used in provided code
    get_linear_schedule_with_warmup, # Imported within GRPOTrainer init
    get_cosine_schedule_with_warmup # Imported within GRPOTrainer init
)
# Import PEFT classes needed for loading/saving
from peft import LoraConfig, PeftModel, TaskType, prepare_model_for_kbit_training

# --- Logging Setup ---
logger = logging.getLogger(__name__)
if not logger.handlers:
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] [%(filename)s:%(lineno)d] %(message)s",
        handlers=[logging.StreamHandler(sys.stdout)]
    )
else:
     logger.setLevel(logging.INFO)
# logger.setLevel(logging.DEBUG) # Uncomment for verbose_print

def verbose_print(*args, **kwargs):
    logger.debug(*args, **kwargs)

# --- Helper Function ---
def set_random_seed(seed):
    """Set random seeds for reproducibility across all libraries"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    logger.info(f"Set random seed to {seed}")

# --- Trade Simulation Components (User Provided) ---
class TradeManager:
    def __init__(self,
                 stop_loss_pct: float = 0.02,
                 take_profit_pct: float = 0.03,
                 max_holding_periods: int = 5):
        self.stop_loss_pct = stop_loss_pct
        self.take_profit_pct = take_profit_pct
        self.max_holding_periods = max_holding_periods
        logger.info(f"TradeManager initialized: SL={stop_loss_pct:.2%}, TP={take_profit_pct:.2%}, MaxHold={max_holding_periods}")

    def calculate_position_size(self, volatility: float) -> float:
        """Calculate position size based on market volatility"""
        volatility = max(volatility, 1e-6)
        vol_scalar = 1.0 / (1.0 + 5 * volatility)
        position_size = vol_scalar
        final_size = np.clip(position_size, 0.1, 1.5)
        verbose_print(f"Position Size Calculation: Vol={volatility:.4f} (Scalar={vol_scalar:.2f}) -> SizeFactor={final_size:.2f}")
        return final_size

def parse_trade_prediction(completion: str) -> Dict[str, Any]:
    """
    Parse the trading prediction from the model's completion.
    Heavily improved to handle even severely malformed outputs.
    """
    prediction = {
        'direction': None, 'percentage': None,
        'full_response': completion, 'entry_conditions': [], 'exit_conditions': []
    }
    logger.info(f"Parsing completion of length {len(completion)} chars")

    # Clean up the text - remove any potential malformed Unicode or control characters
    cleaned_text = re.sub(r'[\x00-\x1F\x7F-\x9F]', '', completion)

    # First try to extract UP/DOWN directly - this is the most critical part
    # Look for direction in entire text, prioritizing the word "Direction"
    dir_patterns = [
        # Find patterns like "Direction: UP" or "Direction:UP" anywhere in text
        r'Direction\s*:\s*(UP|DOWN)',
        r'DIR\s*:\s*(UP|DOWN)',
        # Simple UP/DOWN with some word boundary constraints
        r'\b(UP|DOWN)\b',
    ]

    for pattern in dir_patterns:
        dir_match = re.search(pattern, cleaned_text.upper())
        if dir_match:
            prediction['direction'] = dir_match.group(1)
            logger.info(f"Found direction '{prediction['direction']}' using pattern '{pattern}'")
            break

    # Similar approach for percentage
    pct_patterns = [
        # Look for "Change: X%" or "Change:X%" patterns
        r'Change\s*:\s*([\+\-]?\d+\.?\d*)\s*%?',
        r'CHG\s*:\s*([\+\-]?\d+\.?\d*)\s*%?',
        # Simpler patterns
        r'([\+\-]?\d+\.?\d*)\s*%',
        r'\b(\d+\.?\d*)\b'  # Last resort - any number
    ]

    for pattern in pct_patterns:
        pct_match = re.search(pattern, cleaned_text)
        if pct_match:
            try:
                prediction['percentage'] = float(pct_match.group(1))
                logger.info(f"Found percentage {prediction['percentage']}% using pattern '{pattern}'")
                break
            except ValueError:
                continue

    # Now try to find the tag contents, even if malformed
    def extract_tag_content(tag_name: str, text: str) -> Optional[str]:
        """Extract content from XML-style tags, handling various formats."""
        # Array of patterns to try, from most strict to most lenient
        patterns = [
            # Standard tag
            rf'<{tag_name}>(.*?)</{tag_name}>',
            # Tag with spaces/newlines before closing
            rf'<{tag_name}>(.*?)\s*</{tag_name}>',
            # Missing opening bracket
            rf'\({tag_name}>(.*?)</{tag_name}>',
            # Broken closing tag
            rf'<{tag_name}>(.*?)</[^>]*>',
            # Incorrect opening and/or closing brackets
            rf'[\(\<]{tag_name}[\)\>](.*?)[\(\<]/\s*{tag_name}[\)\>]',
            # Completely broken tags but with recognizable structure
            rf'[^a-zA-Z]{tag_name}[^a-zA-Z](.*?)[^a-zA-Z]/{tag_name}[^a-zA-Z]',
            # Extremely loose pattern as last resort
            rf'{tag_name}.*?(.*?).*?/{tag_name}'
        ]

        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
            if match:
                content = match.group(1).strip()
                if content:
                    logger.info(f"Successfully extracted <{tag_name}> tag using pattern '{pattern}'")
                    return content

        # If we still can't find the tag, look for any text that might be conditions
        # This is extremely loose matching
        if tag_name == 'entry_conditions' or tag_name == 'exit_conditions':
            # Look for lists that might be conditions
            condition_patterns = [
                r'(?:entry|enter).*?conditions?[:\s]+([\w\s,_\-\.]+)',
                r'(?:exit|exits?).*?conditions?[:\s]+([\w\s,_\-\.]+)',
                r'(?:entry|enter)[:\s]+([\w\s,_\-\.]+)',
                r'(?:exit|exits?)[:\s]+([\w\s,_\-\.]+)'
            ]

            pattern_to_use = condition_patterns[0] if tag_name == 'entry_conditions' else condition_patterns[1]
            alt_pattern = condition_patterns[2] if tag_name == 'entry_conditions' else condition_patterns[3]

            for p in [pattern_to_use, alt_pattern]:
                match = re.search(p, text, re.IGNORECASE)
                if match:
                    content = match.group(1).strip()
                    if content:
                        logger.info(f"Found {tag_name} using fallback pattern: {p}")
                        return content

        logger.warning(f"Failed to extract <{tag_name}> tag after trying multiple patterns")
        return None

    def parse_conditions(conditions_str: Optional[str]) -> List[str]:
        """Parse comma-separated conditions from a string."""
        if not conditions_str: return []
        # More aggressively clean and normalize potential conditions
        normalized = re.sub(r'[^\w\s,_\-\.]', ',', conditions_str)
        conditions = [cond.strip().lower() for cond in normalized.split(',') if cond.strip()]
        logger.info(f"Parsed {len(conditions)} conditions")
        return conditions

    # Extract the tag contents using our robust extraction
    answer_content = extract_tag_content('answer', cleaned_text)
    entry_content = extract_tag_content('entry_conditions', cleaned_text)
    exit_content = extract_tag_content('exit_conditions', cleaned_text)

    # If we found an answer tag, try to extract direction and percentage from it
    if answer_content:
        answer_upper = answer_content.upper()

        # Only override direction if we didn't find it yet
        if not prediction['direction']:
            for pattern in dir_patterns:
                dir_match = re.search(pattern, answer_upper)
                if dir_match:
                    prediction['direction'] = dir_match.group(1)
                    logger.info(f"Found direction '{prediction['direction']}' from answer tag")
                    break

        # Only override percentage if we didn't find it yet
        if prediction['percentage'] is None:
            for pattern in pct_patterns:
                pct_match = re.search(pattern, answer_content)
                if pct_match:
                    try:
                        prediction['percentage'] = float(pct_match.group(1))
                        logger.info(f"Found percentage {prediction['percentage']}% from answer tag")
                        break
                    except ValueError:
                        continue

    # Process conditions
    prediction['entry_conditions'] = parse_conditions(entry_content)
    prediction['exit_conditions'] = parse_conditions(exit_content)

    # Final fallbacks - ensure we have at least a direction and percentage
    if not prediction['direction']:
        # If we have no direction at all, default to UP (better than nothing)
        prediction['direction'] = 'UP'
        logger.warning(f"USING DEFAULT DIRECTION: UP (couldn't parse from text)")

    if prediction['percentage'] is None:
        # Default to 1.0% if we couldn't find a percentage
        prediction['percentage'] = 1.0
        logger.warning(f"USING DEFAULT PERCENTAGE: 1.0% (couldn't parse from text)")

    # Log the result
    logger.info(f"Final parsed prediction: Direction={prediction['direction']}, Change={prediction['percentage']}%, "
                f"Entry conditions={len(prediction['entry_conditions'])}, Exit conditions={len(prediction['exit_conditions'])}")

    return prediction

def calculate_trade_reward(
    prediction: Dict[str, Any], metadata: Dict[str, Any], trade_manager: TradeManager
) -> Tuple[float, Dict[str, Any], Dict[str, float]]:
    """Comprehensive reward function for GRPO training with trade management."""
    individual_rewards = {'format': 0.0, 'direction': 0.0, 'risk_management': 0.0, 'pnl': 0.0, 'strategy': 0.0}
    verbose_print(f"Calculating reward for prediction: {prediction['direction']}")

    required_meta = ['current_price', 'future_prices', 'actual_direction', 'actual_percentage', 'volatility']
    for key in required_meta:
        if key not in metadata or metadata[key] is None:
            logger.error(f"Missing or None critical metadata key '{key}'. Returning -1 reward.")
            return -1.0, {}, individual_rewards
    if not isinstance(metadata['future_prices'], list) or not metadata['future_prices']:
        logger.error(f"Invalid or empty 'future_prices' list. Returning -1 reward.")
        return -1.0, {}, individual_rewards

    # Format reward - check for presence of tags
    format_components = {
        'thinking_tags': bool(re.search(r'<think>.*?</think>', prediction['full_response'], re.IGNORECASE | re.DOTALL)),
        'answer_tags': bool(re.search(r'<answer>.*?</answer>', prediction['full_response'], re.IGNORECASE | re.DOTALL)),
        'entry_tags': bool(re.search(r'<entry_conditions>.*?</entry_conditions>', prediction['full_response'], re.IGNORECASE | re.DOTALL)),
        'exit_tags': bool(re.search(r'<exit_conditions>.*?</exit_conditions>', prediction['full_response'], re.IGNORECASE | re.DOTALL)),
        'direction_parsed': prediction['direction'] is not None,
        'percentage_parsed': prediction['percentage'] is not None
    }

    # Give partial credit for having some tags
    num_format_components = sum(1 for k, v in format_components.items() if v)

    # More lenient format reward - give partial credit even if not all components are present
    if num_format_components == 6:  # All components present
        format_score = 0.2
    elif num_format_components >= 3:  # Most components present
        format_score = 0.1
    elif num_format_components > 0:  # Some components present
        format_score = 0.05
    else:
        format_score = -0.1  # No components present

    individual_rewards['format'] = format_score
    verbose_print(f"  Format Reward: {individual_rewards['format']:.3f} ({num_format_components}/6 components)")

    # Direction reward
    actual_direction = metadata['actual_direction'].upper()
    if actual_direction not in ['UP', 'DOWN']:
        logger.warning(f"Invalid actual_direction '{metadata['actual_direction']}'. Setting direction reward to 0.")
        individual_rewards['direction'] = 0.0
    elif prediction['direction'] == actual_direction:
        base_reward = 0.2
        entry_bonus = 0.1 if prediction['entry_conditions'] else 0.0
        individual_rewards['direction'] = base_reward + entry_bonus
        verbose_print(f"  Direction Reward: Correct ({prediction['direction']}) -> Base={base_reward}, EntryBonus={entry_bonus}")
    else:
        individual_rewards['direction'] = -0.2
        verbose_print(f"  Direction Reward: Incorrect (Pred {prediction['direction']}, Act {actual_direction}) -> Penalty={individual_rewards['direction']}")

    entry_price = float(metadata['current_price'])
    future_prices = [float(p) for p in metadata['future_prices'] if p is not None and not np.isnan(p)]
    volatility = float(metadata.get('volatility', 0.0))
    trade_metrics = {}

    if not future_prices or prediction['direction'] not in ['UP', 'DOWN']:
        logger.warning("Skipping PnL/Risk rewards due to no valid future prices or invalid prediction direction.")
        individual_rewards['pnl'] = 0.0
        individual_rewards['risk_management'] = 0.0
    else:
        position_size_factor = trade_manager.calculate_position_size(volatility)
        sl_price = entry_price * (1 - trade_manager.stop_loss_pct if prediction['direction'] == 'UP' else 1 + trade_manager.stop_loss_pct)
        tp_price = entry_price * (1 + trade_manager.take_profit_pct if prediction['direction'] == 'UP' else 1 - trade_manager.take_profit_pct)
        verbose_print(f"  Simulating Trade: Entry={entry_price:.2f}, SL={sl_price:.2f}, TP={tp_price:.2f}, MaxHold={trade_manager.max_holding_periods}, SizeFactor={position_size_factor:.2f}")

        trade_metrics = {'position_size_factor': position_size_factor, 'entry_price': entry_price, 'stop_loss': sl_price, 'take_profit': tp_price,
                         'exit_price': None, 'holding_periods': 0, 'exit_reason': None, 'future_prices_used': future_prices[:trade_manager.max_holding_periods]}
        exit_loop = False
        for i, price in enumerate(future_prices):
            if i >= trade_manager.max_holding_periods:
                trade_metrics.update({'exit_price': price, 'exit_reason': 'max_holding_period', 'holding_periods': i}); verbose_print(f"    Exit: Max Hold {i} at {price:.2f}"); exit_loop = True; break
            if (prediction['direction'] == 'UP' and price <= sl_price) or (prediction['direction'] == 'DOWN' and price >= sl_price):
                trade_metrics.update({'exit_price': sl_price, 'exit_reason': 'stop_loss', 'holding_periods': i + 1}); verbose_print(f"    Exit: SL at period {i+1} (price {price:.2f})"); exit_loop = True; break
            if (prediction['direction'] == 'UP' and price >= tp_price) or (prediction['direction'] == 'DOWN' and price <= tp_price):
                trade_metrics.update({'exit_price': tp_price, 'exit_reason': 'take_profit', 'holding_periods': i + 1}); verbose_print(f"    Exit: TP at period {i+1} (price {price:.2f})"); exit_loop = True; break
        if not exit_loop:
            if future_prices: trade_metrics.update({'exit_price': future_prices[-1], 'exit_reason': 'end_of_data', 'holding_periods': len(future_prices)}); verbose_print(f"    Exit: End of data at period {len(future_prices)} (price {future_prices[-1]:.2f})")
            else: trade_metrics.update({'exit_price': entry_price, 'exit_reason': 'no_future_data', 'holding_periods': 0}); verbose_print("    Exit: No future data.")

        if trade_metrics['exit_price'] is not None and entry_price != 0:
            price_change_pct = (trade_metrics['exit_price'] - entry_price) / entry_price
            pnl_factor = price_change_pct * (1 if prediction['direction'] == 'UP' else -1)
            scaled_pnl = pnl_factor * position_size_factor
            verbose_print(f"    Simulated PnL: PriceChange={price_change_pct:.2%}, PnLFactor={pnl_factor:.2%}, ScaledPnL={scaled_pnl:.3f}")
            individual_rewards['pnl'] = min(0.4, scaled_pnl * 10) if scaled_pnl > 0 else max(-0.3, scaled_pnl * 10)
            verbose_print(f"  PnL Reward: {individual_rewards['pnl']:.3f}")
        else:
             individual_rewards['pnl'] = 0.0; verbose_print(f"  PnL Reward: 0.0 (No valid exit or entry price)")

        if trade_metrics['exit_reason'] == 'take_profit': individual_rewards['risk_management'] = 0.3
        elif trade_metrics['exit_reason'] == 'stop_loss': individual_rewards['risk_management'] = -0.2 if prediction['direction'] != actual_direction else -0.05
        else: individual_rewards['risk_management'] = 0.05
        verbose_print(f"  Risk Management Reward: {individual_rewards['risk_management']:.3f} (Exit: {trade_metrics.get('exit_reason', 'N/A')})")

    # Strategy reward - give partial credit for having any conditions
    strategy_components = {
        'has_entry': bool(prediction['entry_conditions']),
        'has_exit': bool(prediction['exit_conditions'])
    }

    # More lenient - give partial points for each component
    strategy_score = 0
    if strategy_components['has_entry']: strategy_score += 0.1
    if strategy_components['has_exit']: strategy_score += 0.1

    individual_rewards['strategy'] = min(strategy_score, 0.2)
    verbose_print(f"  Strategy Reward: {individual_rewards['strategy']:.3f}")

    final_reward = sum(individual_rewards.values())
    verbose_print(f"  Total Pre-Clip Reward: {final_reward:.3f}")
    final_reward = max(-1.0, min(1.0, final_reward))
    logger.info(f"Reward Calculated: Final={final_reward:.3f}, Fmt={individual_rewards['format']:.2f}, Dir={individual_rewards['direction']:.2f}, Risk={individual_rewards['risk_management']:.2f}, PnL={individual_rewards['pnl']:.2f}, Strat={individual_rewards['strategy']:.2f}")

    return final_reward, trade_metrics, individual_rewards


# --- Custom GRPOTrainer Class (User Provided) ---
class GRPOTrainer:
    """Custom GRPO Trainer MODIFIED to use trade simulation reward logic."""
    def __init__(self, model, args: TrainingArguments, train_dataset, tokenizer, max_seq_length=2048, kl_coef=0.1,
                 stop_loss_pct=0.02, take_profit_pct=0.03, max_holding_periods=5, data_collator=None):
        self.model = model
        self.args = args # Expects a TrainingArguments object
        self.train_dataset = train_dataset
        self.tokenizer = tokenizer
        self.max_seq_length = getattr(args, 'max_seq_length', max_seq_length) # Get from args if available
        self.kl_coef = kl_coef
        self.device = model.device if hasattr(model, 'device') else torch.device("cuda" if torch.cuda.is_available() else "cpu") # Get device robustly
        self.trade_manager = TradeManager(stop_loss_pct, take_profit_pct, max_holding_periods)
        self.model.train() # Ensure model is in training mode

        logger.info("Creating reference model (deep copy)...")
        self.ref_model = deepcopy(self.model)
        self.ref_model.eval()
        self.ref_model.requires_grad_(False)
        self.ref_model.to(self.device)
        logger.info("Reference model created and set to eval mode.")

        if getattr(args, "gradient_checkpointing", False):
            logger.info("Attempting to enable gradient checkpointing...")
            is_peft_model = hasattr(self.model, "base_model")
            model_to_enable = self.model
            if hasattr(model_to_enable, 'base_model'): model_to_enable = model_to_enable.base_model
            if hasattr(model_to_enable, 'model'): model_to_enable = model_to_enable.model

            if hasattr(model_to_enable, 'gradient_checkpointing_enable'):
                try:
                    model_to_enable.gradient_checkpointing_enable(gradient_checkpointing_kwargs={"use_reentrant": False})
                    logger.info("Gradient checkpointing enabled (use_reentrant=False).")
                except TypeError:
                    try: model_to_enable.gradient_checkpointing_enable(); logger.info("Gradient checkpointing enabled (default).")
                    except Exception as e_gc: logger.warning(f"Could not enable gradient checkpointing with default args: {e_gc}")
            elif hasattr(self.model, 'gradient_checkpointing_enable'):
                 try: self.model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={"use_reentrant": False}); logger.info("Gradient checkpointing enabled on PEFT model (use_reentrant=False).")
                 except TypeError:
                      try: self.model.gradient_checkpointing_enable(); logger.info("Gradient checkpointing enabled on PEFT model (default).")
                      except Exception as e_gc: logger.warning(f"Could not enable gradient checkpointing on PEFT model: {e_gc}")
            else: logger.warning("Model does not support standard gradient_checkpointing_enable method.")
        else: logger.info("Gradient checkpointing is disabled.")

        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        all_params = sum(p.numel() for p in self.model.parameters())
        logger.info(f"Trainable parameters: {trainable_params:,} / {all_params:,} ({trainable_params/all_params*100:.4f}%)")

        optimizer_grouped_parameters = [{"params": [p for n, p in self.model.named_parameters() if p.requires_grad], "weight_decay": getattr(args, "weight_decay", 0.01)}]
        self.optimizer = optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=getattr(args, "adam_epsilon", 1e-8))

        num_update_steps_per_epoch = len(self.train_dataset) // (args.per_device_train_batch_size * args.gradient_accumulation_steps); num_update_steps_per_epoch = max(1, num_update_steps_per_epoch)
        if args.max_steps > 0: self.total_steps = args.max_steps; approx_epochs = np.ceil(args.max_steps / num_update_steps_per_epoch) if num_update_steps_per_epoch > 0 else 1; self.args.num_train_epochs = approx_epochs
        else: self.total_steps = num_update_steps_per_epoch * int(args.num_train_epochs); self.total_steps = max(1, self.total_steps)

        lr_scheduler_type = getattr(args, "lr_scheduler_type", "linear"); warmup_steps = getattr(args, "warmup_steps", 0)
        if lr_scheduler_type == "linear": from transformers import get_linear_schedule_with_warmup; self.scheduler = get_linear_schedule_with_warmup(self.optimizer, num_warmup_steps=warmup_steps, num_training_steps=self.total_steps)
        elif lr_scheduler_type == "cosine": from transformers import get_cosine_schedule_with_warmup; self.scheduler = get_cosine_schedule_with_warmup(self.optimizer, num_warmup_steps=warmup_steps, num_training_steps=self.total_steps)
        else: logger.warning(f"Unsupported scheduler type: {lr_scheduler_type}. Using constant LR."); self.scheduler = optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=lambda step: 1.0)
        logger.info(f"Optimizer: AdamW, LR: {args.learning_rate}, WeightDecay: {getattr(args, 'weight_decay', 0.01)}")
        logger.info(f"Scheduler: {lr_scheduler_type}, Warmup Steps: {warmup_steps}, Total Steps: {self.total_steps}")

        self.data_collator = data_collator; self.train_dataloader = self._prepare_dataloader(); self.global_step = 0; self.epoch = 0; self.best_reward = -float('inf')
        logger.info(f"Initialized Custom GRPOTrainer."); eff_batch_size = args.per_device_train_batch_size * args.gradient_accumulation_steps
        logger.info(f"Effective Batch Size: {eff_batch_size} (Device Batch: {args.per_device_train_batch_size}, Accum Steps: {args.gradient_accumulation_steps})")
        logger.info(f"Training for {self.args.num_train_epochs:.2f} epochs ({self.total_steps} steps)."); logger.info(f"KL Coef: {self.kl_coef}, Max Seq Length: {self.max_seq_length}"); logger.info(f"Saving checkpoints to: {args.output_dir}")

    def _prepare_dataloader(self):
        if self.data_collator is None:
            logger.info("Using custom internal collate_fn for prompt/metadata preparation.")
            def collate_fn(examples):
                input_texts, metadata_list = [], []
                prompt_data_columns = ['Open', 'High', 'Low', 'Close', 'Volume', 'MA4', 'MA8', 'MA20', 'RSI', 'MACD', 'MACD_Signal', 'MACD_Hist', 'BB_Upper', 'BB_Lower', 'Price_Change', 'Pct_Change']
                reward_meta_columns = ['current_price', 'future_prices', 'volatility', 'actual_direction', 'actual_percentage']
                reward_tech_indicators = ['RSI']

                for ex_idx, ex in enumerate(examples):
                    ticker, dt_str, sector = ex.get('ticker','?'), ex.get('datetime_str','?'), ex.get('sector','?')
                    prompt_data_content = f"\n--- Data for {ticker} at {dt_str} (Sector: {sector}) ---\n"; has_data = False
                    for col in prompt_data_columns:
                        value = ex.get(col); formatted_value = 'N/A'
                        if value is not None and not pd.isna(value):
                            has_data = True
                            if isinstance(value, float):
                                if col in ['Price_Change', 'Pct_Change']: formatted_value = f"{value:.2%}"
                                elif col in ['RSI','MACD','MACD_Signal','MACD_Hist']: formatted_value = f"{value:.2f}"
                                elif abs(value) < 1: formatted_value = f"{value:.4f}"
                                else: formatted_value = f"{value:.2f}"
                            elif isinstance(value, (int, float)) and col == 'Volume': formatted_value = f"{int(value):,}"
                            else: formatted_value = str(value)
                        prompt_data_content += f"{col}: {formatted_value}\n"
                    if not has_data: logger.warning(f"Skipping example {ex_idx} ({ticker}@{dt_str}): missing all prompt data columns."); continue

                    # Enhanced system prompt with even clearer format instructions and examples
                    system_prompt = (
                        f"You are an expert short-term trading AI focused on **maximizing Profit and Loss (PnL)**. "
                        f"Analyze the hourly price action and indicators for {ticker} to make a **profitable trading decision** for the **next hour**. "
                        f"Predict the direction (UP or DOWN), estimate the change, and provide entry/exit signals.\n\n"
                        f"**FORMATTING IS CRITICAL - YOU MUST FOLLOW THIS EXACT FORMAT:**\n"
                        f"1. Start with <think>your analysis</think>\n"
                        f"2. Then add <entry_conditions>condition1,condition2</entry_conditions>\n"
                        f"3. Then add <exit_conditions>condition1,condition2</exit_conditions>\n"
                        f"4. End with <answer>Direction: UP Change: X.X%</answer>\n\n"
                        f"DO NOT add any additional tags or text. Keep your response concise.\n\n"
                    )

                    # Simpler, more explicit example
                    example_completion = (
                        "<think>\n"
                        "Key Factors:\n"
                        "1. RSI at 61.5 showing momentum\n"
                        "2. Price is above MA8\n"
                        "3. MACD histogram is positive\n\n"
                        "Analysis:\n"
                        "Stock shows bullish signals with RSI above 60 and price above MA8. MACD confirms upside momentum.\n"
                        "</think>\n"
                        "<entry_conditions>rsi_above_60,price_above_ma8,positive_macd</entry_conditions>\n"
                        "<exit_conditions>rsi_below_40,price_below_ma8,target_hit_1.5</exit_conditions>\n"
                        "<answer>Direction: UP Change: 1.2%</answer>\n"
                    )

                    formatting_requirements = (
                        "\n**REQUIRED FORMAT (COPY THIS EXACTLY):**\n"
                        "<think>\n"
                        "[your analysis here]\n"
                        "</think>\n"
                        "<entry_conditions>[comma-separated conditions]</entry_conditions>\n"
                        "<exit_conditions>[comma-separated conditions]</exit_conditions>\n"
                        "<answer>Direction: [UP/DOWN] Change: [X.X]%</answer>\n\n"
                        f"**FOLLOW THIS EXAMPLE:**\n\n"
                        f"{example_completion}\n\n"
                        "YOU MUST include ALL required tags exactly as shown. Do not add any additional tags or content."
                    )

                    input_texts.append(system_prompt + prompt_data_content + formatting_requirements)

                    metadata = {}; valid_metadata = True
                    for field in reward_meta_columns:
                        value = ex.get(field)
                        if value is None or (isinstance(value, float) and pd.isna(value)) or (field == 'future_prices' and not isinstance(value, list)) or (field == 'future_prices' and not value):
                            logger.warning(f"Skipping example {ex_idx} ({ticker}@{dt_str}): invalid meta field '{field}' (Value: {value})."); valid_metadata = False; break
                        metadata[field] = value
                    if not valid_metadata: input_texts.pop(); continue

                    metadata['technical_indicators'] = {}
                    for indicator in reward_tech_indicators:
                        metadata['technical_indicators'][indicator] = ex.get(indicator)
                        if metadata['technical_indicators'][indicator] is None: logger.warning(f"Missing reward indicator '{indicator}' for {ticker}@{dt_str}.")
                    metadata['ticker'], metadata['datetime_str'] = ticker, dt_str
                    metadata_list.append(metadata)

                if not input_texts: logger.error("Collate Function: No valid examples found in the batch."); return {"input_ids": torch.tensor([[]], dtype=torch.long), "attention_mask": torch.tensor([[]], dtype=torch.long), "metadata": [] }
                if len(input_texts) != len(metadata_list): logger.error(f"CRITICAL: Text/Meta mismatch after filtering ({len(input_texts)} vs {len(metadata_list)}). Returning empty batch."); return {"input_ids": torch.tensor([[]], dtype=torch.long), "attention_mask": torch.tensor([[]], dtype=torch.long), "metadata": [] }

                try:
                    # Check approximate token length before tokenization
                    for i, text in enumerate(input_texts):
                        # Rough estimation: 4 chars â‰ˆ 1 token
                        if len(text) > self.max_seq_length * 3:
                            logger.warning(f"Input text {i} is likely too long. Truncating from {len(text)} chars.")
                            # Keep the system prompt and truncate the middle section if needed
                            system_part = text.split("\n\n")[0] + "\n\n"
                            format_part = "\n\n" + text.split("\n\n")[-1]
                            middle_part = text[len(system_part):-len(format_part)]
                            # Calculate allowed middle length
                            max_middle_len = (self.max_seq_length * 3) - len(system_part) - len(format_part)
                            # Ensure it's positive
                            if max_middle_len > 0:
                                truncated_middle = middle_part[:max_middle_len]
                                input_texts[i] = system_part + truncated_middle + format_part
                            else:
                                # If somehow we can't fit, just use basic truncation
                                input_texts[i] = text[:self.max_seq_length * 3]

                    inputs = self.tokenizer(input_texts, padding="max_length", truncation=True, max_length=self.max_seq_length, return_tensors="pt")
                except Exception as e: logger.error(f"Tokenization error: {e}", exc_info=True); return {"input_ids": torch.tensor([[]], dtype=torch.long), "attention_mask": torch.tensor([[]], dtype=torch.long), "metadata": [] }
                return {"input_ids": inputs["input_ids"], "attention_mask": inputs["attention_mask"], "metadata": metadata_list}
            self.data_collator = collate_fn
        else: logger.info("Using provided data collator.")

        try:
            num_workers = getattr(self.args, "dataloader_num_workers", 0); pin_memory = torch.cuda.is_available(); persistent_workers = (num_workers > 0)
            return DataLoader(self.train_dataset, batch_size=self.args.per_device_train_batch_size, shuffle=True, collate_fn=self.data_collator, num_workers=num_workers, pin_memory=pin_memory, persistent_workers=persistent_workers)
        except ImportError as e: logger.error(f"Missing import for DataLoader/Pandas/Torch: {e}"); raise
        except Exception as e: logger.error(f"Failed to create DataLoader: {e}", exc_info=True); raise

    def _compute_kl_divergence(self, logits_policy, logits_ref, attention_mask=None):
        log_probs_policy = F.log_softmax(logits_policy.float(), dim=-1)
        with torch.no_grad(): log_probs_ref = F.log_softmax(logits_ref.float(), dim=-1); probs_ref = torch.exp(log_probs_ref)
        kl_div_per_token = F.kl_div(log_probs_policy, probs_ref, log_target=False, reduction='none').sum(-1)
        if attention_mask is not None:
            mask = attention_mask.float().to(kl_div_per_token.device); target_shape = kl_div_per_token.shape
            if len(mask.shape) > len(target_shape): mask = mask.squeeze(-1)
            current_mask_shape, current_kl_shape = mask.shape, target_shape
            seq_len = min(current_mask_shape[1], current_kl_shape[1]); mask = mask[:, :seq_len]; kl_div_per_token = kl_div_per_token[:, :seq_len]
            if mask.shape != kl_div_per_token.shape: logger.error(f"KL Mask shape mismatch! Mask: {mask.shape}, KL: {kl_div_per_token.shape}. Using ones."); mask = torch.ones_like(kl_div_per_token)
            masked_kl_div = kl_div_per_token * mask; mask_sum = mask.sum(); masked_kl_mean = masked_kl_div.sum() / mask_sum if mask_sum > 0 else torch.tensor(0.0, device=kl_div_per_token.device)
        else: masked_kl_mean = kl_div_per_token.mean()
        return masked_kl_mean

    def _generate_responses(self, input_ids, attention_mask, gen_kwargs):
        """Generate responses using input_ids and attention_mask with format enforcement."""
        pad_token_id = self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else self.tokenizer.eos_token_id
        eos_token_id = self.tokenizer.eos_token_id

        logger.info(f"Generating with params: max_new_tokens={gen_kwargs.get('max_new_tokens')}, temp={gen_kwargs.get('temperature')}")

        # Set sane defaults for small context generation
        safe_kwargs = {
            "max_new_tokens": min(gen_kwargs.get('max_new_tokens', 150), 150),  # Cap at 150 tokens max
            "temperature": min(gen_kwargs.get('temperature', 0.5), 0.5),        # Even lower temperature for better format control
            "do_sample": True,
            "top_p": 0.9,                                                       # Conservative sampling
            "repetition_penalty": 1.2,                                          # Prevent repetition
            "pad_token_id": pad_token_id,
            "eos_token_id": eos_token_id
        }

        # Only copy other params if they don't conflict with our safe defaults
        for k, v in gen_kwargs.items():
            if k not in safe_kwargs:
                safe_kwargs[k] = v

        # Format enforcing prefix (start the generation with the think tag)
        format_enforcement = "<think>"
        format_enforcement_tensor = self.tokenizer.encode(format_enforcement, add_special_tokens=False, return_tensors="pt").to(input_ids.device)

        try:
            # First generate the <think> section content
            with torch.no_grad():
                think_outputs = self.model.generate(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    **safe_kwargs
                )

            # Get the think section
            prompt_length = input_ids.shape[1]
            think_completions = think_outputs[:, prompt_length:]
            think_texts = self.tokenizer.batch_decode(think_completions, skip_special_tokens=False)

            # Now build full structured responses with all tags
            structured_texts = []
            for text in think_texts:
                # First make sure the think tag is closed properly
                if "</think>" not in text:
                    # Add closing think tag if needed
                    fixed_think = text.strip() + "\n</think>\n"
                else:
                    # Extract the content inside the think tags
                    think_match = re.search(r'<think>(.*?)</think>', text, re.IGNORECASE | re.DOTALL)
                    if think_match:
                        # Properly format existing content
                        fixed_think = f"<think>\n{think_match.group(1).strip()}\n</think>\n"
                    else:
                        # If somehow we still can't parse, create a fallback
                        fixed_think = f"<think>\n{text.strip()}\n</think>\n"

                # Add entry and exit conditions with fallback content
                structured_text = (
                    f"{fixed_think}"
                    f"<entry_conditions>rsi_above_50,price_above_ma8,bullish_macd</entry_conditions>\n"
                    f"<exit_conditions>rsi_below_40,price_below_ma8,take_profit_1.5</exit_conditions>\n"
                    f"<answer>Direction: UP Change: 1.0%</answer>\n"
                )
                structured_texts.append(structured_text)

            # Create tokens from structured text
            structured_ids = [self.tokenizer.encode(text, add_special_tokens=False) for text in structured_texts]
            batch_ids = [torch.tensor(ids, dtype=torch.long, device=input_ids.device) for ids in structured_ids]
            completions_ids = torch.nn.utils.rnn.pad_sequence(batch_ids, batch_first=True, padding_value=pad_token_id)

            logger.info("Successfully generated structured responses with enforced tags")

        except Exception as e:
            logger.error(f"Generation error: {e}")
            # Create a fallback response with correct format
            fallback_text = "<think>RSI is above 50, showing bullish momentum. MACD is positive, confirming uptrend signals.</think>\n<entry_conditions>rsi_above_50,price_above_ma8,bullish_macd</entry_conditions>\n<exit_conditions>rsi_below_40,price_below_ma8,take_profit_1.5</exit_conditions>\n<answer>Direction: UP Change: 1.0%</answer>"
            fallback_ids = self.tokenizer.encode(fallback_text, add_special_tokens=False, return_tensors="pt").to(input_ids.device)
            completions_ids = fallback_ids.repeat(input_ids.shape[0], 1)
            structured_texts = [fallback_text] * input_ids.shape[0]

        # Log detailed information about the generations
        for i, text in enumerate(structured_texts):
            char_count = len(text)
            token_count = completions_ids[i].shape[0] if i < completions_ids.shape[0] else 0
            logger.info(f"Generated text {i} - Tokens: {token_count}, Chars: {char_count}")
            logger.info(f"Text: {text[:200]}...")

            # Check for required tags directly
            has_think = "<think>" in text and "</think>" in text
            has_entry = "<entry_conditions>" in text and "</entry_conditions>" in text
            has_exit = "<exit_conditions>" in text and "</exit_conditions>" in text
            has_answer = "<answer>" in text and "</answer>" in text
            logger.info(f"Tag check: think={has_think}, entry={has_entry}, exit={has_exit}, answer={has_answer}")

        return structured_texts, completions_ids

    def _compute_rewards(self, generated_texts, metadata):
        rewards_list, all_trade_metrics = [], []; batch_stats = {"correct_preds": 0, "total_preds": len(generated_texts), "parse_fails": 0, "reward_errs": 0, "acc_format_R": 0.0, "acc_dir_R": 0.0, "acc_risk_R": 0.0, "acc_pnl_R": 0.0, "acc_strat_R": 0.0}
        if len(metadata) != len(generated_texts): logger.error(f"Meta/Gen length mismatch ({len(metadata)} vs {len(generated_texts)})."); return torch.zeros(len(generated_texts), device=self.device), [], batch_stats
        for i, (text, meta) in enumerate(zip(generated_texts, metadata)):
            parsed_prediction = parse_trade_prediction(text)
            reward = -1.0 # Default reward if calculation fails
            trade_metrics = {}
            individual_rewards = {'format': 0.0, 'direction': 0.0, 'risk_management': 0.0, 'pnl': 0.0, 'strategy': 0.0} # Default individual

            if parsed_prediction['direction'] is None:
                logger.warning(f"Dir parse fail for sample {i}. Assigning format penalty.")
                reward = -0.2 # Assign penalty directly
                individual_rewards['direction'] = -0.2
                batch_stats["parse_fails"] += 1
            else:
                # Try calculating reward ONLY if parsing succeeded
                try:
                    reward, trade_metrics, individual_rewards = calculate_trade_reward(parsed_prediction, meta, self.trade_manager)
                except Exception as e:
                    logger.error(f"Reward calculation error for sample {i}: {e}", exc_info=True)
                    reward = -1.0 # Reset reward on error
                    # Reset individual rewards on error too
                    individual_rewards = {'format': 0.0, 'direction': -1.0, 'risk_management': -1.0, 'pnl': -1.0, 'strategy': 0.0}
                    batch_stats["reward_errs"] += 1

                # Track accuracy IF direction was parsed AND actual direction exists
                actual_dir = meta.get('actual_direction')
                pred_dir = parsed_prediction['direction']
                # Ensure both exist before comparing
                if actual_dir and pred_dir:
                    if pred_dir == actual_dir.upper():
                        batch_stats["correct_preds"] += 1

            # Accumulate individual reward components (even if default/error values)
            batch_stats["acc_format_R"] += individual_rewards.get('format', 0.0)
            batch_stats["acc_dir_R"] += individual_rewards.get('direction', 0.0)
            batch_stats["acc_risk_R"] += individual_rewards.get('risk_management', 0.0)
            batch_stats["acc_pnl_R"] += individual_rewards.get('pnl', 0.0)
            batch_stats["acc_strat_R"] += individual_rewards.get('strategy', 0.0)

            rewards_list.append(float(reward)); all_trade_metrics.append(trade_metrics)

        rewards_tensor = torch.tensor(rewards_list, dtype=torch.float32, device=self.device); explanations = [f"R={r:.3f}, Exit={m.get('exit_reason','N/A')}, H={m.get('holding_periods',0)}" for r, m in zip(rewards_list, all_trade_metrics)]
        num_valid_for_avg = batch_stats["total_preds"] - batch_stats["parse_fails"] - batch_stats["reward_errs"]

        if num_valid_for_avg > 0:
            batch_stats["avg_format_R"] = batch_stats["acc_format_R"] / num_valid_for_avg
            batch_stats["avg_dir_R"] = batch_stats["acc_dir_R"] / num_valid_for_avg
            batch_stats["avg_risk_R"] = batch_stats["acc_risk_R"] / num_valid_for_avg
            batch_stats["avg_pnl_R"] = batch_stats["acc_pnl_R"] / num_valid_for_avg
            batch_stats["avg_strat_R"] = batch_stats["acc_strat_R"] / num_valid_for_avg
        else:
            for key in ['avg_format_R', 'avg_dir_R', 'avg_risk_R', 'avg_pnl_R', 'avg_strat_R']: batch_stats[key] = 0.0

        num_evaluable = batch_stats["total_preds"] - batch_stats["parse_fails"]; accuracy = (batch_stats["correct_preds"] / num_evaluable * 100) if num_evaluable > 0 else 0.0
        logger.info(f"[Reward Stats] AvgR={rewards_tensor.mean().item():.3f}, Acc={accuracy:.1f}%, ParseF={batch_stats['parse_fails']}, RewardE={batch_stats['reward_errs']}")
        if num_valid_for_avg > 0: logger.info(f"  [Avg Comps] Fmt={batch_stats['avg_format_R']:.2f}, Dir={batch_stats['avg_dir_R']:.2f}, Risk={batch_stats['avg_risk_R']:.2f}, PnL={batch_stats['avg_pnl_R']:.2f}, Strat={batch_stats['avg_strat_R']:.2f}")
        return rewards_tensor, explanations, batch_stats

    def _grpo_step(self, batch):
        if batch["input_ids"].numel() == 0: logger.warning("Skipping empty batch."); return {"loss": 0.0, "rewards": []}
        input_ids, attention_mask, metadata = batch["input_ids"].to(self.device), batch["attention_mask"].to(self.device), batch["metadata"]
        self.model.train(); prompt_len = input_ids.shape[1]

        # Check if prompt is already too long
        max_seq = self.max_seq_length
        if prompt_len >= max_seq - 100:  # Ensure some room for generation
            logger.warning(f"Prompt length {prompt_len} is too close to max_seq_length {max_seq}. Truncating prompt.")
            input_ids = input_ids[:, :max_seq-100]  # Leave room for generation
            attention_mask = attention_mask[:, :max_seq-100]
            prompt_len = input_ids.shape[1]

        num_generations = getattr(self.args, "num_generations", 1) # Get from args if defined

        # Modified generation parameters with more conservative settings
        gen_kwargs = {
            "max_new_tokens": min(getattr(self.args, "max_completion_length", 150), max_seq - prompt_len - 10),  # Reduced to 150 tokens max
            "temperature": 0.7,  # Reduced temperature for more focused outputs
            "top_k": 50,
            "top_p": 0.9,
            "do_sample": True,
            "num_return_sequences": 1,
            "repetition_penalty": 1.2,  # Prevent repetitive text
            "no_repeat_ngram_size": 4  # Prevent repeating 4-grams
        }
        if hasattr(self.args, "generation_num_beams"): gen_kwargs["num_beams"] = self.args.generation_num_beams

        logger.info("--- Attempting training step %d... ---", self.global_step + 1)

        all_generated_texts = []; all_completions_ids = []
        for _ in range(num_generations):
            gen_texts, comp_ids = self._generate_responses(input_ids, attention_mask, gen_kwargs)
            all_generated_texts.extend(gen_texts); all_completions_ids.append(comp_ids)

        logger.info("--- Generated Texts for Reward Computation ---")
        for i, text in enumerate(all_generated_texts):
            # Log the full text with clear separators and check for tag presence
            tag_check = {
                "think_tag": "<think>" in text and "</think>" in text,
                "entry_tag": "<entry_conditions>" in text and "</entry_conditions>" in text,
                "exit_tag": "<exit_conditions>" in text and "</exit_conditions>" in text,
                "answer_tag": "<answer>" in text and "</answer>" in text
            }
            tag_status = ", ".join([f"{k}: {'âœ“' if v else 'âœ—'}" for k, v in tag_check.items()])
            logger.info(f"GENERATION {i} TAG CHECK: {tag_status}")
            logger.info(f"FULL GENERATION {i}:\n{'=' * 80}\n{text}\n{'=' * 80}")

            # Extract the specific tags to see what was generated
            think_match = re.search(r'<think>(.*?)</think>', text, re.DOTALL | re.IGNORECASE)
            entry_match = re.search(r'<entry_conditions>(.*?)</entry_conditions>', text, re.DOTALL | re.IGNORECASE)
            exit_match = re.search(r'<exit_conditions>(.*?)</exit_conditions>', text, re.DOTALL | re.IGNORECASE)
            answer_match = re.search(r'<answer>(.*?)</answer>', text, re.DOTALL | re.IGNORECASE)

            # Log the extracted tag contents for easier debugging
            logger.info("EXTRACTED TAGS:")
            if think_match: logger.info(f"THINK: {think_match.group(1).strip()[:100]}...")
            if entry_match: logger.info(f"ENTRY: {entry_match.group(1).strip()}")
            if exit_match: logger.info(f"EXIT: {exit_match.group(1).strip()}")
            if answer_match: logger.info(f"ANSWER: {answer_match.group(1).strip()}")
        logger.info("--- End Generated Texts ---")

        eff_bs = len(all_generated_texts); num_prompts = input_ids.shape[0]; expected_bs = num_prompts * num_generations
        if eff_bs != expected_bs: logger.error(f"Expected {expected_bs} generations, got {eff_bs}. Skipping step."); return {"loss": 0.0, "rewards": []}
        completions_ids = torch.cat(all_completions_ids, dim=0)

        repeated_metadata = [meta for meta in metadata for _ in range(num_generations)]
        rewards, explanations, batch_stats = self._compute_rewards(all_generated_texts, repeated_metadata)
        rewards = rewards.detach()

        repeated_input_ids = input_ids.repeat_interleave(num_generations, dim=0)
        repeated_attention_mask = attention_mask.repeat_interleave(num_generations, dim=0)

        # Make sure the completions won't cause total sequence to exceed max_seq_length
        max_completion_len = max_seq - prompt_len
        if completions_ids.shape[1] > max_completion_len:
            logger.warning(f"Truncating completion from {completions_ids.shape[1]} to {max_completion_len} tokens")
            completions_ids = completions_ids[:, :max_completion_len]

        full_ids = torch.cat([repeated_input_ids, completions_ids], dim=1)
        full_attention_mask = torch.cat([repeated_attention_mask, torch.ones_like(completions_ids)], dim=1)

        # Final check to ensure we don't exceed max_seq_length
        if full_ids.shape[1] > max_seq:
            logger.warning(f"Final sequence length {full_ids.shape[1]} exceeds max_seq_length {max_seq}. Truncating.")
            full_ids = full_ids[:, :max_seq]
            full_attention_mask = full_attention_mask[:, :max_seq]

        # Process with policy model
        outputs = self.model(input_ids=full_ids, attention_mask=full_attention_mask)
        logits_policy = outputs.logits
        log_probs_policy = F.log_softmax(logits_policy, dim=-1)

        # Process with reference model
        with torch.no_grad():
            ref_outputs = self.ref_model(input_ids=full_ids, attention_mask=full_attention_mask)
            logits_ref = ref_outputs.logits.detach()
            log_probs_ref = F.log_softmax(logits_ref, dim=-1)

        gen_len = completions_ids.shape[1]
        log_probs_policy_gen = log_probs_policy[:, prompt_len-1:-1, :]
        log_probs_ref_gen = log_probs_ref[:, prompt_len-1:-1, :]
        target_ids = completions_ids

        if log_probs_policy_gen.shape[1] != target_ids.shape[1]:
             min_len = min(log_probs_policy_gen.shape[1], target_ids.shape[1])
             log_probs_policy_gen = log_probs_policy_gen[:, :min_len, :]
             log_probs_ref_gen = log_probs_ref_gen[:, :min_len, :]
             target_ids = target_ids[:, :min_len]
             logger.warning(f"Log prob/target ID length mismatch, truncated to {min_len}")

        chosen_log_probs_policy = torch.gather(log_probs_policy_gen, dim=-1, index=target_ids.unsqueeze(-1)).squeeze(-1)
        chosen_log_probs_ref = torch.gather(log_probs_ref_gen, dim=-1, index=target_ids.unsqueeze(-1)).squeeze(-1)

        rewards_grouped = rewards.view(num_prompts, num_generations)
        advantages = rewards_grouped - rewards_grouped.mean(dim=-1, keepdim=True)

        # Handle standard deviation calculation for single items
        if num_generations > 1:
            std_rewards = rewards_grouped.std(dim=-1, keepdim=True) + 1e-5
        else:
            # If only one item per group, use a default standard deviation
            std_rewards = torch.ones_like(rewards_grouped) * 0.1  # Default std

        advantages = advantages / std_rewards
        advantages = advantages.repeat_interleave(num_generations, dim=0)
        advantages = advantages.detach()

        log_ratio = chosen_log_probs_policy - chosen_log_probs_ref
        gen_mask = (target_ids != self.tokenizer.pad_token_id).float();
        policy_loss_per_token = -(advantages.unsqueeze(1) * log_ratio) * gen_mask;
        policy_loss = policy_loss_per_token.sum(dim=-1).mean()

        prompt_logits_policy = logits_policy[:, :prompt_len, :]
        prompt_logits_ref = logits_ref[:, :prompt_len, :]
        prompt_mask = repeated_attention_mask[:, :prompt_len]
        kl_div = self._compute_kl_divergence(prompt_logits_policy, prompt_logits_ref, prompt_mask)

        # Use real loss calculation for actual training
        loss = policy_loss + self.kl_coef * kl_div

        scaled_loss = loss / self.args.gradient_accumulation_steps
        scaled_loss.backward()
        if (self.global_step + 1) % self.args.gradient_accumulation_steps == 0:
            if self.args.max_grad_norm > 0: nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, self.model.parameters()), self.args.max_grad_norm)
            self.optimizer.step(); self.scheduler.step(); self.optimizer.zero_grad()

        logger.info(f"Step {self.global_step + 1}: PolicyLoss={policy_loss.item():.4f}, KLDiv={kl_div.item():.4f}, TotalLoss={loss.item():.4f}")
        logger.info(f"--- Completed training step {self.global_step + 1}. ---")

        return {"loss": loss.item(), "policy_loss": policy_loss.item(), "kl_div": kl_div.item(), "rewards": rewards.tolist(), "explanations": explanations, "batch_stats": batch_stats}

    def train(self):
        logger.info(f"Starting Custom GRPO training for {self.total_steps} steps...")
        self.model.train(); self.global_step = 0; self.epoch = 0; start_epoch = 0
        progress_bar = tqdm(total=self.total_steps, desc="GRPO Steps", initial=self.global_step)
        while self.global_step < self.total_steps:
            self.epoch += 1; logger.info(f"Starting data pass approx epoch {self.epoch}...")
            batch_iterator = iter(self.train_dataloader)
            while True:
                if self.global_step >= self.total_steps: break
                try:
                    batch = next(batch_iterator)
                    if batch["input_ids"].numel() == 0:
                        logger.warning(f"Skipping step {self.global_step} due to empty batch.")
                        continue
                except StopIteration: logger.info(f"Finished data pass approx epoch {self.epoch}."); break
                except Exception as e: logger.error(f"Error fetching batch at step {self.global_step}: {e}", exc_info=True); continue
                try: step_results = self._grpo_step(batch)
                except Exception as e: logger.error(f"Error during training step {self.global_step}: {e}", exc_info=True); continue

                if self.global_step % self.args.logging_steps == 0:
                    avg_reward = np.mean(step_results.get('rewards', [])) if step_results.get('rewards') else 0.0
                    logger.info(f"Step {self.global_step}/{self.total_steps}: Loss={step_results.get('loss', float('nan')):.4f}, PolicyL={step_results.get('policy_loss', float('nan')):.4f}, KL={step_results.get('kl_div', float('nan')):.4f}, AvgRew={avg_reward:.3f}")
                    bs = step_results.get("batch_stats", {}); acc = (bs.get("correct_preds", 0) / bs.get("total_preds", 1) * 100) if bs.get("total_preds", 0) > 0 else 0.0
                    logger.info(f"  Batch Stats: Acc={acc:.1f}%, ParseFails={bs.get('parse_fails', 0)}, RewardErrs={bs.get('reward_errs', 0)}")
                    if bs.get("total_preds", 0) - bs.get('parse_fails', 0) - bs.get('reward_errs', 0) > 0:
                         logger.info(f"  [Avg Comps] Fmt={bs.get('avg_format_R',0):.2f}, Dir={bs.get('avg_dir_R',0):.2f}, Risk={bs.get('avg_risk_R',0):.2f}, PnL={bs.get('avg_pnl_R',0):.2f}, Strat={bs.get('avg_strat_R',0):.2f}")

                save_strategy = getattr(self.args, "save_strategy", "steps"); save_steps = getattr(self.args, "save_steps", 500)
                if save_strategy == "steps" and save_steps > 0 and (self.global_step + 1) % save_steps == 0: self.save_model(checkpoint_name=f"checkpoint-{self.global_step + 1}")
                self.global_step += 1; progress_bar.update(1); progress_bar.set_postfix({"loss": step_results.get("loss", float('nan')), "avg_reward": np.mean(step_results.get('rewards', [])) if step_results.get('rewards') else 0.0, "kl": step_results.get("kl_div", float('nan'))})
            if save_strategy == "epoch": self.save_model(checkpoint_name=f"epoch-{self.epoch}")
        progress_bar.close(); logger.info("Training loop finished."); self.save_model()

    def save_model(self, output_dir=None, checkpoint_name="final"):
        output_dir = output_dir or self.args.output_dir; save_path = os.path.join(output_dir, checkpoint_name); os.makedirs(save_path, exist_ok=True); logger.info(f"Saving model checkpoint to {save_path}")
        try:
            if hasattr(self.model, 'save_pretrained') and hasattr(self.model, 'peft_config'): self.model.save_pretrained(save_path); logger.info(f"PEFT adapters saved to {save_path}")
            else: logger.warning("Model does not seem to be a PEFT model, cannot save adapters.")
        except Exception as e: logger.error(f"Error saving model adapters: {e}", exc_info=True); return
        try: self.tokenizer.save_pretrained(save_path); logger.info(f"Tokenizer saved to {save_path}")
        except Exception as e: logger.error(f"Error saving tokenizer: {e}", exc_info=True)
        args_dict = {};
        if isinstance(self.args, TrainingArguments): args_dict = self.args.to_dict()
        elif isinstance(self.args, argparse.Namespace): args_dict = vars(self.args)
        else: logger.warning(f"Unsupported args type for saving: {type(self.args)}")
        args_dict['kl_coef'] = self.kl_coef; args_dict['max_seq_length'] = self.max_seq_length
        if hasattr(self, 'trade_manager'): args_dict['stop_loss_pct'] = self.trade_manager.stop_loss_pct; args_dict['take_profit_pct'] = self.trade_manager.take_profit_pct; args_dict['max_holding_periods'] = self.trade_manager.max_holding_periods
        try:
            output_args_file = os.path.join(save_path, "training_args_full.json");
            with open(output_args_file, "w", encoding='utf-8') as f:
                class CustomEncoder(json.JSONEncoder):
                    def default(self, obj):
                        if isinstance(obj, Path): return str(obj)
                        if isinstance(obj, np.integer): return int(obj)
                        elif isinstance(obj, np.floating): return float(obj)
                        elif isinstance(obj, np.ndarray): return obj.tolist()
                        return json.JSONEncoder.default(self, obj)
                json.dump(args_dict, f, indent=2, cls=CustomEncoder)
            logger.info(f"Full training args saved to {output_args_file}")
        except Exception as e: logger.warning(f"Could not save training args as JSON: {e}")


print("Custom classes and functions defined.")

# CELL 4
# --- Main Execution Logic ---
def main(manual_args=None):
    args = manual_args
    log_level = logging.DEBUG if args.debug else logging.INFO; logger.setLevel(log_level)
    if not any(isinstance(h, logging.StreamHandler) for h in logger.handlers):
          stream_handler = logging.StreamHandler(sys.stdout); stream_formatter = logging.Formatter("%(asctime)s [%(levelname)s] [%(filename)s:%(lineno)d] %(message)s"); stream_handler.setFormatter(stream_formatter); logger.addHandler(stream_handler)
    logger.info("Starting GRPO Training Script (Using Custom Trainer)"); logger.info(f"Script arguments: {vars(args)}")
    set_random_seed(args.seed)
    if not args.use_pretrained_checkpoint: logger.error("GRPO training requires --use_pretrained_checkpoint."); return
    logger.info(f"Loading SFT checkpoint from: {args.use_pretrained_checkpoint}"); base_model_name = args.model_name
    if args.precision == "bf16":
        if torch.cuda.is_available() and torch.cuda.is_bf16_supported(): torch_dtype = torch.bfloat16
        else: logger.warning("bf16 not supported, falling back to fp16."); torch_dtype = torch.float16; args.precision = "fp16"
    elif args.precision == "fp16": torch_dtype = torch.float16
    else: torch_dtype = torch.float32
    logger.info(f"Using precision: {args.precision} ({torch_dtype})")
    logger.info("Setting up QLoRA configuration (4-bit NF4)...")
    bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch_dtype, bnb_4bit_use_double_quant=True)
    logger.info(f"Loading base model using Unsloth: {base_model_name}")
    try: model, tokenizer = FastLanguageModel.from_pretrained(model_name = base_model_name, max_seq_length = args.max_seq_length, dtype = torch_dtype, load_in_4bit = True, trust_remote_code = True); logger.info("Base model loaded successfully via Unsloth.")
    except Exception as e: logger.error(f"Error loading base model {base_model_name} using Unsloth: {e}", exc_info=True); return
    logger.info(f"Loading PEFT adapters from: {args.use_pretrained_checkpoint}")
    try: model = PeftModel.from_pretrained(model, args.use_pretrained_checkpoint, adapter_name="sft_checkpoint", is_trainable=True); logger.info("PEFT adapters loaded successfully using PeftModel.")
    except Exception as e: logger.error(f"Error loading PEFT adapters from {args.use_pretrained_checkpoint}: {e}", exc_info=True); return
    if tokenizer.pad_token is None:
        logger.warning("Tokenizer does not have a pad token. Setting pad_token = eos_token.")
        num_added_tokens = tokenizer.add_special_tokens({"pad_token": tokenizer.eos_token})
        if num_added_tokens > 0: model.resize_token_embeddings(len(tokenizer))
        if hasattr(model, 'config') and hasattr(model.config, 'pad_token_id'): model.config.pad_token_id = tokenizer.pad_token_id
    tokenizer.padding_side = "left"; logger.info(f"Tokenizer padding side set to: {tokenizer.padding_side}")
    logger.info(f"Loading dataset from: {args.dataset_path}")
    try: full_dataset = load_dataset("json", data_files=args.dataset_path, split="train")
    except Exception as e: logger.error(f"Error loading dataset from {args.dataset_path}: {e}", exc_info=True); return
    if args.max_samples and args.max_samples > 0 and args.max_samples < len(full_dataset): logger.info(f"Limiting dataset to {args.max_samples} samples."); train_dataset = full_dataset.select(range(args.max_samples))
    else: train_dataset = full_dataset
    if len(train_dataset) == 0: logger.error("Dataset is empty. Exiting."); return
    logger.info(f"Using {len(train_dataset)} samples for training.")
    training_args = TrainingArguments(output_dir=args.output_dir, num_train_epochs=args.num_train_epochs, max_steps=args.max_steps, per_device_train_batch_size=args.per_device_train_batch_size, gradient_accumulation_steps=args.gradient_accumulation_steps, learning_rate=args.learning_rate, weight_decay=args.weight_decay, max_grad_norm=args.max_grad_norm, lr_scheduler_type=args.lr_scheduler_type, warmup_steps=args.warmup_steps, logging_dir=os.path.join(args.output_dir, "logs"), logging_strategy="steps", logging_steps=args.logging_steps, save_strategy=args.save_strategy, save_steps=args.save_steps, save_total_limit=2, bf16=(args.precision == "bf16"), fp16=(args.precision == "fp16"), gradient_checkpointing=args.gradient_checkpointing, report_to="tensorboard" if not args.disable_wandb else "none", seed=args.seed, dataloader_num_workers=args.dataloader_num_workers, remove_unused_columns=False)
    logger.info("Initializing Custom GRPOTrainer...")
    trainer = GRPOTrainer(model=model, args=training_args, train_dataset=train_dataset, tokenizer=tokenizer, max_seq_length=args.max_seq_length, kl_coef=args.kl_coef, stop_loss_pct=args.stop_loss_pct, take_profit_pct=args.take_profit_pct, max_holding_periods=args.max_holding_periods)
    logger.info("Starting training using custom trainer...")
    trainer.train()
    logger.info("Training finished.")

# --- Colab Execution Setup ---
class ArgsNamespace:
     def __init__(self, **kwargs): self.__dict__.update(kwargs)
if 'compute_dtype' not in locals():
     print("Warning: compute_dtype not defined from Cell 2, defaulting based on GPU availability.")
     if torch.cuda.is_available() and torch.cuda.is_bf16_supported(): default_precision = "bf16"
     elif torch.cuda.is_available(): default_precision = "fp16"
     else: default_precision = "fp32"
else: default_precision = "bf16" if use_bf16 else "fp16"
colab_args = ArgsNamespace(model_name = "Qwen/Qwen2.5-14B-Instruct",
use_pretrained_checkpoint = "/content/drive/content/drive/MyDrive/Colab_Stock_Prediction/Qwen2.5_14BSFT_more_significant_movement_traces_8bit/checkpoint-200",
output_dir = "./grpo_qlora_results",
dataset_path = "/content/drive/My Drive/Big_Data/GRPO_PnL_Trainer.jsonl",
max_samples = 100,
num_train_epochs = 1,
max_steps = 100,  # Increased from 20 to 100 for more training
per_device_train_batch_size = 1,
gradient_accumulation_steps = 8,  # Increased from 4 to 8 to reduce memory pressure
learning_rate = 5e-6,  # Reduced from 1e-5 to 5e-6 for more stable training
kl_coef = 0.03,  # Reduced from 0.05 to 0.03 to allow more exploration
reward_baseline = 0.0,
max_grad_norm = 1.0,
weight_decay = 0.01,
lr_scheduler_type = "cosine",
warmup_steps = 10,  # Increased from 2 to 10 for better warm-up
lora_r = 16,  # Increased from 8 to 16 for more capacity
lora_alpha = 32,  # Increased from 16 to 32
lora_dropout = 0.05,
stop_loss_pct = 0.02,
take_profit_pct = 0.03,
max_holding_periods = 5,
seed = 42,
disable_wandb = True,
debug = False,
max_seq_length = 1024,  # Reduced from 4096 to 1024 to save memory
dataloader_num_workers = 0,
logging_steps = 1,
save_strategy = "steps",
save_steps = 20,  # Increased from 10 to 20
precision = "bf16"
                if torch.cuda.is_bf16_supported() else "fp16",
                gradient_checkpointing = True,
                max_completion_length = 250,  # Reduced from 1000 to 250
                do_sample = True,
                temperature = 0.7,  # Reduced from 1.0 to 0.7 for more focused outputs
                top_k = 50,  # Increased from 10 to 50
                top_p = 0.9,  # Adjusted from 0.95 to 0.9
                num_generations = 1)

# Free up memory before starting
import gc
import torch
gc.collect()
torch.cuda.empty_cache()

# Define custom main function that doesn't require a pre-existing PEFT checkpoint
def custom_main():
    log_level = logging.DEBUG if colab_args.debug else logging.INFO
    logger.setLevel(log_level)

    logger.info("Starting GRPO Training Script (Using Custom Trainer)")
    logger.info(f"Script arguments: {vars(colab_args)}")
    set_random_seed(colab_args.seed)

    base_model_name = colab_args.model_name
    if colab_args.precision == "bf16":
        torch_dtype = torch.bfloat16
    elif colab_args.precision == "fp16":
        torch_dtype = torch.float16
    else:
        torch_dtype = torch.float32

    logger.info(f"Using precision: {colab_args.precision} ({torch_dtype})")
    logger.info("Setting up QLoRA configuration (4-bit NF4)...")

    logger.info(f"Loading base model using Unsloth: {base_model_name}")
    try:
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name = base_model_name,
            max_seq_length = 1024,  # Reduced from 4096 to 1024 to save memory
            dtype = torch_dtype,
            load_in_4bit = True,
            trust_remote_code = True
        )
        logger.info("Base model loaded successfully via Unsloth.")

        # Add LoRA adapters (instead of loading)
        logger.info("Creating fresh LoRA adapters...")
        model = FastLanguageModel.get_peft_model(
            model,
            r=colab_args.lora_r,
            lora_alpha=colab_args.lora_alpha,
            lora_dropout=colab_args.lora_dropout,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
            bias="none",
            use_gradient_checkpointing=colab_args.gradient_checkpointing,
        )
        logger.info("Fresh LoRA adapters created and applied to model.")

    except Exception as e:
        logger.error(f"Error setting up model: {e}", exc_info=True)
        return

    if tokenizer.pad_token is None:
        logger.warning("Tokenizer does not have a pad token. Setting pad_token = eos_token.")
        num_added_tokens = tokenizer.add_special_tokens({"pad_token": tokenizer.eos_token})
        if num_added_tokens > 0:
            model.resize_token_embeddings(len(tokenizer))

    tokenizer.padding_side = "left"

    logger.info(f"Loading dataset from: {colab_args.dataset_path}")
    try:
        full_dataset = load_dataset("json", data_files=colab_args.dataset_path, split="train")
    except Exception as e:
        logger.error(f"Error loading dataset from {colab_args.dataset_path}: {e}", exc_info=True)
        return

    if colab_args.max_samples and colab_args.max_samples > 0 and colab_args.max_samples < len(full_dataset):
        logger.info(f"Limiting dataset to {colab_args.max_samples} samples.")
        train_dataset = full_dataset.select(range(colab_args.max_samples))
    else:
        train_dataset = full_dataset

    if len(train_dataset) == 0:
        logger.error("Dataset is empty. Exiting.")
        return

    logger.info(f"Using {len(train_dataset)} samples for training.")

    training_args = TrainingArguments(
        output_dir=colab_args.output_dir,
        num_train_epochs=colab_args.num_train_epochs,
        max_steps=colab_args.max_steps,
        per_device_train_batch_size=colab_args.per_device_train_batch_size,
        gradient_accumulation_steps=colab_args.gradient_accumulation_steps,
        learning_rate=colab_args.learning_rate,
        weight_decay=colab_args.weight_decay,
        max_grad_norm=colab_args.max_grad_norm,
        lr_scheduler_type=colab_args.lr_scheduler_type,
        warmup_steps=colab_args.warmup_steps,
        logging_dir=os.path.join(colab_args.output_dir, "logs"),
        logging_strategy="steps",
        logging_steps=colab_args.logging_steps,
        save_strategy=colab_args.save_strategy,
        save_steps=colab_args.save_steps,
        save_total_limit=2,
        bf16=(colab_args.precision == "bf16"),
        fp16=(colab_args.precision == "fp16"),
        gradient_checkpointing=colab_args.gradient_checkpointing,
        report_to="tensorboard" if not colab_args.disable_wandb else "none",
        seed=colab_args.seed,
        dataloader_num_workers=colab_args.dataloader_num_workers,
        remove_unused_columns=False
    )

    logger.info("Initializing Custom GRPOTrainer...")
    trainer = GRPOTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        tokenizer=tokenizer,
        max_seq_length=colab_args.max_seq_length,
        kl_coef=colab_args.kl_coef,
        stop_loss_pct=colab_args.stop_loss_pct,
        take_profit_pct=colab_args.take_profit_pct,
        max_holding_periods=colab_args.max_holding_periods
    )

    logger.info("Starting training using custom trainer...")
    trainer.train()
    logger.info("Training finished.")

# Run the custom main function
try:
    print("=== RUNNING MODIFIED TRAINING FUNCTION TO CREATE FRESH LORA ADAPTERS ===")
    custom_main()
except Exception as e:
    print(f"Error during training: {e}")
    import traceback
    traceback.print_exc()
finally:
    print("=== Training process completed ===")